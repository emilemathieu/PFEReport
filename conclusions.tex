%!TEX root = internshipReport.tex

\chapter{Conclusion and personal review}

\section{Conclusion}
After giving some background on the mission, we reviewed well-known \acrlong{BNP} models, with a focus on infinite mixture models and discrete random probability measures. After we reviewed the design of \acrlongpl{PPL} and described the framework in which inference schemes can be generically applied for these programs.
Then we recalled a generative construction of \acrlongpl{PKP} called \acrlong{SBS} and stressed out that it relates with a high order \glspl{PPL}, via stochastic recursion.
We implemented such as a generative process for some \gls{BNP} classes, along with a mixture model in a \gls{PPL} named Turing, and ran experiments to assess the performance of posterior samplers.

Further work is still needed to design and implement an effective posterior sampler.
Furthermore, this project could lead to other interesting ideas. Such as performing inference whithin a \gls{PPL} with a Piecewise Deterministic \gls{MCMC} scheme, with variational inference or maybe via adversarial inference. Also, enabling a \gls{PPL} to automatically compute gradients via auto-differentation could lead to new models.


\section{Personal review}
I am much satisfied by this internship which brought me much experience and knowledge.
I worked during four months on a subject involving Bayesian nonparametrics, Computational Statistics and Programing Languages, while my knowledge of these fields was limited. I therefore had to learn a lot about these, obviously by reading articles and books, but also by actually implementing some inference schemes. Talking with other students, post-docs and professors is also a stimulating way to get to know more about other related subjects. I was for instance invited by Frank Wood \footnote{He works on \acrlongpl{PPL}, \url{http://www.robots.ox.ac.uk/~fwood/}} in the Department of Engineering \footnote{\url{http://www.eng.ox.ac.uk}} to have a talk with him and some of his students. Reading groups are good places to frequently meet with the other students and keep up with the state of the art. I have been leading the probabilistic inference reading group \footnote{\url{https://github.com/BigBayes/oxsml/wiki/Probabilistic-Inference-meetings}} and will also participate to the Reinforcement Learning reading group \footnote{\url{http://www.stats.ox.ac.uk/~cmaddis/}} organised by Chris Maddison.

I also have much appreciated the great opportunities brought by this internship/PhD position, such as OxCSML \footnote{Oxford Computational Statistics \& Machine Learning} weekly talks \footnote{\url{http://csml.stats.ox.ac.uk/events/}} or other department's talks. I particularly enjoyed Judith Rousseau's \footnote{\url{https://www.ceremade.dauphine.fr/~rousseau/}} talk on posterior consistency and Jim Pitman's \footnote{\url{https://www.stat.berkeley.edu/~pitman/}} talk on transformations of Brownian processes. I have also been invited for a week to the Microsoft Research AI Summer School 2017 \footnote{\url{https://www.microsoft.com/en-us/research/event/ai-summer-school-2017/}} in Cambridge, where I met many other PhD students working in machine learning related areas.

Moreover, I made lots of progress in my research workflow. Before, coming here, even if I had already worked on several so called \textit{research projects}, I had never actually done any research. I am still at the beginning of my research ``career'', yet I believe to better understand the spirit of research; iterating back and forth between questions and answers, by experimenting, reading, writing and discussing with others.
I have learned to enjoy and take the most of the freedom and autonomy I possess by being a PhD student, which was quite destabilizing at first.
