\documentclass[twoside,10pt,openany,a4paper]{rapport}

\input{preamble/preamble}
\input{preamble/preamble_acronyms}
\input{preamble/preamble_math}
%\input{preamble/preamble_tikz}

\usepackage{algpseudocode,algorithm,algorithmicx}  
\usepackage{caption}
\usepackage{subcaption}

\graphicspath{{images/}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

\begin{document}

%Non affiché mais sera inséré dans les propriétés du fichier
\title{Césure}
\author{Émile \textsc{Mathieu}}
\date{\today}

%\mainmatter
\frontmatter

%Page de garde
\begin{titlepage}
      \begin{center}
      \begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{logo_enpc.jpg}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{logo_oxford.png}
\end{subfigure}
\vspace{1.0cm}
\end{figure}

      \vspace{0.3cm}
      \institute{École des Ponts ParisTech}\\
      \institute{Department of Statistics, University of Oxford}
      
      \vspace{0.7cm}
      2017\\
      Master's Internship Report
      
      \vspace{0.3cm}
      Émile Mathieu\\
      Élève-ingénieur, Third year
      
      \vspace{2cm}
      \Huge{\textbf{Bayesian Nonparametric Inference within Probabilistic Programming Languages}}\\

      \vfill
      \normalsize{}
      Internship carried out at Department of Statistics, University of Oxford \\
      %Stages effectués du 6 Juillet au 30 décembre 2015 puis du 11 Janvier au 08 juillet 2016
      From the 22nd of May, to the 15th of September 2017.
      
      \vspace{0.3cm}
       Company tutor: \textsc{Teh}, Yee Whye\\
       Training supervisor: \textsc{Obozinski}, Guillaume

      \end{center}
\end{titlepage}

\cleardoublepage


\chapter{Acknowledgments}

First of all, I would like to express my indebtedness appreciation to my departmental supervisor Prof. Yee Whye Teh. His belief in me and his advices played a decisive role in making the execution of my work and thus this report. \\

I also express my deepest thanks to Benjamin Bloem-Reddy, who as a postdoctoral, oversaw me during this internship and with whom I have continuously worked. \\

Moreover, my gratitude goes to Guillaume Obozinski, my school training supervisor, whose guidance has continually shaped my career path since I have been at Ecole des Ponts ParisTech.

\chapter{Abstract}

On one side, \gls{BNP} models have gained attraction because of their flexibility. These models, which automatically adapt with the number and complexity of data, avoid having to define \textit{a priori} the number of parameters of the model, such as the number of components for a mixture model.
On the other side, \glspl{PPL} allow practitioners to express probabilistic models in a universal way, and bring generic inference algorithms. These systems avoid designing specific inference schemes, which is error-prone and time consuming.
Specific representations of \gls{BNP} models must be used so as to denote such models in \glspl{PPL}, since \gls{BNP} models live in infinite dimensional space and machines only have finite memory and computational resources.
%Secondly, how should the inference schemes be designed so that by using \gls{PPL}s (and thus gaining in flexibility) performance is not too much affected ?

In this report, we review well-known \gls{BNP} models with a focus on mixture models and discrete random probability measures, but we also give background on the design of \glspl{PPL} and associated inference schemes. We utilize generative construction of \glspl{BNP} and show that more generic \gls{BNP} classes than the \acrlong{DP} can be represented in \glspl{PPL}. We prototype our approach by contributing to an existing \gls{PPL}.

\textbf{Keywords :} Probabilistic Programming, Bayesian Non-parametric, Bayesian Inference, Generative process, Sampling methods.


% \chapter{Résumé}

\tableofcontents
\addcontentsline{toc}{chapter}{Table of contents}
%\listoftables
%\addcontentsline{toc}{chapter}{Liste des tableaux}
% \listoffigures
% \addcontentsline{toc}{chapter}{List of figures}

% \chapter{Glossary}

% \begin{itemize} 
% \item \textbf{BNP}:  \textit{Bayesian Non-Parametric}, explained in Section \ref{BNP_def}. \\
% \item \textbf{PPL}:  \textit{Probabilistic Programming Language}, explained in Section \ref{PPL_def}. \\

% \end{itemize}

\mainmatter


\chapter{Introduction}

For data science practitioners, statistical inference is typically just one step in a more elaborate analysis workflow. The first stage of this work involves data acquisition, pre-processing and cleaning. This is often followed by several iterations of exploratory model design and testing of inference algorithms. Once a sufficiently robust statistical model and a corresponding inference algorithm have been identified, analysis results must be post-processed, visualized, and in some cases integrated into a wider production system.
Probabilistic programming systems \cite{Gordon:2014:PP:2593882.2593900,Goodman:2012uq,Mansinghka:2014ty,wood-aistats-2014,Turing} represent generative models as programs written in a specialized language that provides syntax for the definition and conditioning of random variables. The code for such models is generally concise, modular, and easy to modify or extend. Inference can be performed for any probabilistic program using one or more generic inference techniques provided by the system back end. These systems therefore avoid having to design specific inference schemes, which is error-prone and time consuming.

In the previously described data scientist's workflow, model selection is often a particularly difficult step.
Most scientists address this problem by first fitting several models, with different numbers of clusters or factors, and then selecting one using model comparison metrics \cite{Claeskens:1251912}. Model selection metrics usually include two terms. The first term measures how well the model fits the data. The second term, a complexity penalty, favours simpler models (i.e., ones with fewer components or factors).
\acrlongpl{BNP} models provide a different approach to this problem. Rather than comparing models that vary in complexity, the \gls{BNP} approach is to fit a single model that can adapt its complexity to the data. Indeed, they allow the complexity to grow as more data are observed, such as when using a model to perform prediction. This is an attractive property for many settings.
Nonetheless, such models cannot be used out-of the box within \acrlongpl{PPL} since they have by definition at least one infinite dimensional component and computers only have finite memory and computational resources. Consequently, specific representations of these models must be constructed so as to denote them in probabilistic programming systems. Our work focuses on generative constructions of such models, and on developing efficient algorithms to perform inference within \glspl{PPL}.

After giving some background on the Department of Statistics in Chapter 1 we develop on the context and organisation of the project. We then review in Chapter 4, well-known \acrlong{BNP} models, with a focus on infinite mixture models and discrete random probability measures. This chapter also highlights the tailored representations of \gls{BNP} models used by \acrlong{MCMC} samplers in the literature. After that in Chapter 5, we review the design of \acrlongpl{PPL} and describe the framework in which inference schemes can be generically applied for these programs.
We then recall a generative construction of \acrlongpl{PKP} in Chapter 6, and stress out how it relates with a specific class of \glspl{PPL}. We also implement this generative process, along with a mixture model in an already existing \gls{PPL}, and run experiments to assess the performance of posterior samplers.
Finally, in Chapters 7 and 8 we conclude an point out possible directions for future work.

The main contributions of this report are the following:

\begin{itemize}
  \item Being able to sample from some discrete random probability measures in \glspl{PPL}, and therefore represent infinite mixture models.
  \item Efficiently sample from posterior distributions of hidden variables in infinite mixture models.
\end{itemize}

\include{presentation}


\include{mission}



\include{sectionBNP}

% \include{sectionInference}

\include{sectionPPL}


\include{sectionBNPwithPPL}


\include{sectionFutureWork}


\include{conclusions}

%\backmatter

% \appendix
% \chapter*{Appendices}
% \addcontentsline{toc}{chapter}{Appendices}
% \renewcommand{\thesection}{\Alph{section}}

% \section{Variational Inference ?}
% \section{Automatic Differentiation ?}
% \section{Exchangeability} \label{exchangeability}
% The underlying assumption of all Bayesian methods is that the parameter specifying the observation model is a random variable. This assumption is subject to much criticism, and at the heart of the Bayesian versus non-Bayesian debate that has long divided the statistics community. However, there is a very general type of observations for which the existence of such a random variable can be derived mathematically: For so-called \textit{exchangeable} observations, the Bayesian assumption that a randomly distributed parameter exists is not a modeling assumption, but a mathematical consequence of the data’s properties.

% Formally, a sequence of variables $X_1,X_2,\dots,X_n$ over the same probability space $(\mathcal{X},\Omega)$ is \textit{exchangeable} if their joint distribution is invariant to permuting the variables. That is, if $P$ is the joint distribution and $\sigma$ any permutation of $\{1,\dots,n\}$, then
% $$ P(X_1=x_1,\dots,X_n=x_n) = P(X_1=x_{\sigma(1)},\dots,X_n=x_{\sigma(n)}) $$

% An infinite sequence $X_1,X_2,\dots$ is \textit{infinitely exchangeable} if $X_1,\dots,X_n$ is \textit{exchangeable} for every $n \ge 1$. Exchangeability reflects the assumption that the variables do not depend on their indices although they may be dependent among themselves. This is typically a reasonable assumption in machine learning and statistical applications, even if the variables are not themselves iid (independently and identically distributed). Exchangeability is a much weaker assumption than iid since iid variables are automatically exchangeable.

% If $\theta$ parametrizes the underlying distribution, and one assumes a prior distribution over $\theta$, then the resulting marginal distribution over $X_1,X_2,\dots,X_n$ with $\theta$ marginalized out will still be exchangeable. A fundamental result credited to de Finetti \cite{finetti31} states that the converse is also true. That is, if $X_1,X_2,\dots,X_n$ is (infinitely) exchangeable, then there is a random $\theta$ such that:
% \begin{equation} \label{eq:deFineti}
% P(X_1,\dots,X_n) = \int{P(\theta)\prod_{i=1}^n{P(X_i|\theta) d\theta}}
% \end{equation}
% for every $n \ge 1$. In other words, the seemingly innocuous assumption of exchangeability automatically implies the existence of a hierarchical Bayesian model with $\theta$ being the random latent parameter. This the crux of the fundamental importance of exchangeability to Bayesian statistics.

% In de Finetti’s Theorem it is important to stress that $\theta$ can be infinite dimensional (it is typically a random measure), thus the hierarchical Bayesian model \ref{eq:deFineti} is typically a nonparametric one. For example, the Blackwell-MacQueen urn scheme (related to the \gls{CRP}) is exchangeable thus implicitly defines a random measure, namely the \gls{DP}.

%\nocite{*}

\bibliographystyle{siam}
\bibliography{rapport}

\end{document}
