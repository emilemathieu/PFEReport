\documentclass[twoside,10pt,openany,a4paper]{rapport}

\input{preamble/preamble}
\input{preamble/preamble_acronyms}
\input{preamble/preamble_math}
%\input{preamble/preamble_tikz}

\usepackage{algpseudocode,algorithm,algorithmicx}  
\usepackage{caption}
\usepackage{subcaption}

\graphicspath{{images/}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

\begin{document}

%Non affiché mais sera inséré dans les propriétés du fichier
\title{Césure}
\author{Émile \textsc{Mathieu}}
\date{\today}

%\mainmatter
\frontmatter

%Page de garde
\begin{titlepage}
      \begin{center}
      %\includegraphics[scale=0.5]{logo_enpc.jpg}
      %\includegraphics[scale=0.2]{logo_oxford.png}
      \begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{logo_enpc.jpg}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{logo_oxford.png}
\end{subfigure}
\vspace{1.0cm}
\end{figure}

      
      \vspace{0.3cm}
      \institute{École des Ponts ParisTech}\\
      \institute{Department of Statistics, University of Oxford}
      
      \vspace{0.7cm}
      2017\\
      Master's Internship Report
      
      \vspace{0.3cm}
      Émile Mathieu\\
      Élève ingénieur, Third year
      
      \vspace{2cm}
      \Huge{\textbf{Bayesian Nonparametric Inference within Probabilistic Programming Languages}}\\
      %\Large{\textit{within Probabilistic Programming Languages}} 

      %\vspace{2cm}
      %\Huge{\textbf{CONFIDENTIEL}}\\
      
      \vfill
      \normalsize{}
      Internship carried out at Department of Statistics, University of Oxford \\
      %Stages effectués du 6 Juillet au 30 décembre 2015 puis du 11 Janvier au 08 juillet 2016
      From the 22nd of May, to the 15th of September 2017.
      
      \vspace{0.3cm}
       Company tutor: \textsc{Teh}, Yee Whye\\
       Training supervisor: \textsc{Obozinski}, Guillaume

      \end{center}
\end{titlepage}

\cleardoublepage


\chapter{Acknowledgments}

First of all, I would like to express my indebtedness appreciation to my departmental supervisor Prof. Yee Whye Teh. His belief in me and his advices played a decisive role in making the execution of my work and thus this report. \\

I also express my deepest thanks to Benjamin Bloem-Reddy, who as a postdoc, oversaw me during this internship and with whom I frequently work. \\

Moreover, my gratitude goes to Guillaume Obozinski, my school training supervisor, whose guidance has continually shaped my career path since I have been at Ecole des Ponts ParisTech.

\chapter{Abstract}

\gls{BNP} model have been gaining attraction because of their flexibility. Indeed, these models, automatically adapt with the number of data which avoid having to define a priori the number of parameters of the model, such as the number of components for a mixture model for instance.
%+ streaming ?
\gls{PPLs} allow practitioners to express probabilistic programs in a universal way, and bring generic inference algorithms. These systems avoid designing specific inference schemes, which is error-prone and time consuming.

Therefore, a natural question is how can \gls{BNP} models be represented in such systems ? BNP models live in infinite dimensional space which is problematic for machines having finite memory and computational resources.
Secondly, how should the inference schemes be designed so that by using \gls{PPL}s (and thus gaining in flexibility) performance is not too much affected ?

\textbf{Keywords :} Probabilistic Programming, Bayesian Non-parametric, Bayesian Inference, Sampling methods


\chapter{Résumé}
\textcolor{red}{translation of abstract in French}


\tableofcontents
\addcontentsline{toc}{chapter}{Table of contents}
%\listoftables
%\addcontentsline{toc}{chapter}{Liste des tableaux}
\listoffigures
\addcontentsline{toc}{chapter}{List of figures}

\chapter{Glossary}

\begin{itemize} 
\item \textbf{BNP}:  \textit{Bayesian Non-Parametric}, explained in Section \ref{BNP_def}. \\
\item \textbf{PPL}:  \textit{Probabilistic Programming Language}, explained in Section \ref{PPL_def}. \\

\end{itemize}

\mainmatter


\chapter{Introduction}
cf Research Proposal ?
Speak about the PhD which is following
Plan: First, Then, After, Eventually, etc


\include{presentation}


\include{mission}



\include{sectionBNP}

% \include{sectionInference}

\include{sectionPPL}


\include{sectionBNPwithPPL}


\include{sectionFutureWork}


\include{conclusions}

%\backmatter

% \appendix
% \chapter*{Appendices}
% \addcontentsline{toc}{chapter}{Appendices}
% \renewcommand{\thesection}{\Alph{section}}

% \section{Variational Inference ?}
% \section{Automatic Differentiation ?}
% \section{Exchangeability} \label{exchangeability}
% The underlying assumption of all Bayesian methods is that the parameter specifying the observation model is a random variable. This assumption is subject to much criticism, and at the heart of the Bayesian versus non-Bayesian debate that has long divided the statistics community. However, there is a very general type of observations for which the existence of such a random variable can be derived mathematically: For so-called \textit{exchangeable} observations, the Bayesian assumption that a randomly distributed parameter exists is not a modeling assumption, but a mathematical consequence of the data’s properties.

% Formally, a sequence of variables $X_1,X_2,\dots,X_n$ over the same probability space $(\mathcal{X},\Omega)$ is \textit{exchangeable} if their joint distribution is invariant to permuting the variables. That is, if $P$ is the joint distribution and $\sigma$ any permutation of $\{1,\dots,n\}$, then
% $$ P(X_1=x_1,\dots,X_n=x_n) = P(X_1=x_{\sigma(1)},\dots,X_n=x_{\sigma(n)}) $$

% An infinite sequence $X_1,X_2,\dots$ is \textit{infinitely exchangeable} if $X_1,\dots,X_n$ is \textit{exchangeable} for every $n \ge 1$. Exchangeability reflects the assumption that the variables do not depend on their indices although they may be dependent among themselves. This is typically a reasonable assumption in machine learning and statistical applications, even if the variables are not themselves iid (independently and identically distributed). Exchangeability is a much weaker assumption than iid since iid variables are automatically exchangeable.

% If $\theta$ parametrizes the underlying distribution, and one assumes a prior distribution over $\theta$, then the resulting marginal distribution over $X_1,X_2,\dots,X_n$ with $\theta$ marginalized out will still be exchangeable. A fundamental result credited to de Finetti \cite{finetti31} states that the converse is also true. That is, if $X_1,X_2,\dots,X_n$ is (infinitely) exchangeable, then there is a random $\theta$ such that:
% \begin{equation} \label{eq:deFineti}
% P(X_1,\dots,X_n) = \int{P(\theta)\prod_{i=1}^n{P(X_i|\theta) d\theta}}
% \end{equation}
% for every $n \ge 1$. In other words, the seemingly innocuous assumption of exchangeability automatically implies the existence of a hierarchical Bayesian model with $\theta$ being the random latent parameter. This the crux of the fundamental importance of exchangeability to Bayesian statistics.

% In de Finetti’s Theorem it is important to stress that $\theta$ can be infinite dimensional (it is typically a random measure), thus the hierarchical Bayesian model \ref{eq:deFineti} is typically a nonparametric one. For example, the Blackwell-MacQueen urn scheme (related to the \gls{CRP}) is exchangeable thus implicitly defines a random measure, namely the \gls{DP}.

%\nocite{*}

\bibliographystyle{siam}
\bibliography{rapport}

\end{document}
